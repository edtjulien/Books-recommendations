{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fairseq import utils\n",
    "from fairseq.data import encoders\n",
    "\n",
    "\n",
    "class RobertaHubInterface(nn.Module):\n",
    "    \"\"\"A simple PyTorch Hub interface to RoBERTa.\n",
    "    Usage: https://github.com/pytorch/fairseq/tree/main/examples/roberta\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, task, model):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.task = task\n",
    "        self.model = model\n",
    "\n",
    "        self.bpe = encoders.build_bpe(cfg.bpe)\n",
    "\n",
    "        # this is useful for determining the device\n",
    "        self.register_buffer(\"_float_tensor\", torch.tensor([0], dtype=torch.float))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._float_tensor.device\n",
    "\n",
    "    def encode(\n",
    "        self, sentence: str, *addl_sentences, no_separator=False\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        BPE-encode a sentence (or multiple sentences).\n",
    "        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.\n",
    "        Every sentence ends with an end-of-sentence (`</s>`) and we use an\n",
    "        extra end-of-sentence (`</s>`) as a separator.\n",
    "        Example (single sentence): `<s> a b c </s>`\n",
    "        Example (sentence pair): `<s> d e f </s> </s> 1 2 3 </s>`\n",
    "        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE\n",
    "        requires leading spaces. For example::\n",
    "            >>> roberta.encode('Hello world').tolist()\n",
    "            [0, 31414, 232, 2]\n",
    "            >>> roberta.encode(' world').tolist()\n",
    "            [0, 232, 2]\n",
    "            >>> roberta.encode('world').tolist()\n",
    "            [0, 8331, 2]\n",
    "        \"\"\"\n",
    "        bpe_sentence = \"<s> \" + self.bpe.encode(sentence) + \" </s>\"\n",
    "        for s in addl_sentences:\n",
    "            bpe_sentence += \" </s>\" if not no_separator else \"\"\n",
    "            bpe_sentence += \" \" + self.bpe.encode(s) + \" </s>\"\n",
    "        tokens = self.task.source_dictionary.encode_line(\n",
    "            bpe_sentence, append_eos=False, add_if_not_exist=False\n",
    "        )\n",
    "        return tokens.long()\n",
    "\n",
    "    def decode(self, tokens: torch.LongTensor):\n",
    "        assert tokens.dim() == 1\n",
    "        tokens = tokens.numpy()\n",
    "        if tokens[0] == self.task.source_dictionary.bos():\n",
    "            tokens = tokens[1:]  # remove <s>\n",
    "        eos_mask = tokens == self.task.source_dictionary.eos()\n",
    "        doc_mask = eos_mask[1:] & eos_mask[:-1]\n",
    "        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n",
    "        sentences = [\n",
    "            self.bpe.decode(self.task.source_dictionary.string(s)) for s in sentences\n",
    "        ]\n",
    "        if len(sentences) == 1:\n",
    "            return sentences[0]\n",
    "        return sentences\n",
    "\n",
    "    def extract_features(\n",
    "        self, tokens: torch.LongTensor, return_all_hiddens: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        if tokens.dim() == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "        if tokens.size(-1) > self.model.max_positions():\n",
    "            raise ValueError(\n",
    "                \"tokens exceeds maximum length: {} > {}\".format(\n",
    "                    tokens.size(-1), self.model.max_positions()\n",
    "                )\n",
    "            )\n",
    "        features, extra = self.model(\n",
    "            tokens.to(device=self.device),\n",
    "            features_only=True,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        if return_all_hiddens:\n",
    "            # convert from T x B x C -> B x T x C\n",
    "            inner_states = extra[\"inner_states\"]\n",
    "            return [inner_state.transpose(0, 1) for inner_state in inner_states]\n",
    "        else:\n",
    "            return features  # just the last layer's features\n",
    "\n",
    "    def register_classification_head(\n",
    "        self, name: str, num_classes: int = None, embedding_size: int = None, **kwargs\n",
    "    ):\n",
    "        self.model.register_classification_head(\n",
    "            name, num_classes=num_classes, embedding_size=embedding_size, **kwargs\n",
    "        )\n",
    "\n",
    "    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool = False):\n",
    "        features = self.extract_features(tokens.to(device=self.device))\n",
    "        logits = self.model.classification_heads[head](features)\n",
    "        if return_logits:\n",
    "            return logits\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def extract_features_aligned_to_words(\n",
    "        self, sentence: str, return_all_hiddens: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Extract RoBERTa features, aligned to spaCy's word-level tokenizer.\"\"\"\n",
    "        from fairseq.models.roberta import alignment_utils\n",
    "        from spacy.tokens import Doc\n",
    "\n",
    "        nlp = alignment_utils.spacy_nlp()\n",
    "        tokenizer = alignment_utils.spacy_tokenizer()\n",
    "\n",
    "        # tokenize both with GPT-2 BPE and spaCy\n",
    "        bpe_toks = self.encode(sentence)\n",
    "        spacy_toks = tokenizer(sentence)\n",
    "        spacy_toks_ws = [t.text_with_ws for t in tokenizer(sentence)]\n",
    "        alignment = alignment_utils.align_bpe_to_words(self, bpe_toks, spacy_toks_ws)\n",
    "\n",
    "        # extract features and align them\n",
    "        features = self.extract_features(\n",
    "            bpe_toks, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        features = features.squeeze(0)\n",
    "        aligned_feats = alignment_utils.align_features_to_words(\n",
    "            self, features, alignment\n",
    "        )\n",
    "\n",
    "        # wrap in spaCy Doc\n",
    "        doc = Doc(\n",
    "            nlp.vocab,\n",
    "            words=[\"<s>\"] + [x.text for x in spacy_toks] + [\"</s>\"],\n",
    "            spaces=[True]\n",
    "            + [x.endswith(\" \") for x in spacy_toks_ws[:-1]]\n",
    "            + [True, False],\n",
    "        )\n",
    "        assert len(doc) == aligned_feats.size(0)\n",
    "        doc.user_token_hooks[\"vector\"] = lambda token: aligned_feats[token.i]\n",
    "        return doc\n",
    "\n",
    "    def fill_mask(self, masked_input: str, topk: int = 5):\n",
    "        masked_token = \"<mask>\"\n",
    "        assert (\n",
    "            masked_token in masked_input and masked_input.count(masked_token) == 1\n",
    "        ), \"Please add one {0} token for the input, eg: 'He is a {0} guy'\".format(\n",
    "            masked_token\n",
    "        )\n",
    "\n",
    "        text_spans = masked_input.split(masked_token)\n",
    "        text_spans_bpe = (\n",
    "            (\" {0} \".format(masked_token))\n",
    "            .join([self.bpe.encode(text_span.rstrip()) for text_span in text_spans])\n",
    "            .strip()\n",
    "        )\n",
    "        tokens = self.task.source_dictionary.encode_line(\n",
    "            \"<s> \" + text_spans_bpe + \" </s>\",\n",
    "            append_eos=False,\n",
    "            add_if_not_exist=False,\n",
    "        )\n",
    "\n",
    "        masked_index = (tokens == self.task.mask_idx).nonzero(as_tuple=False)\n",
    "        if tokens.dim() == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "\n",
    "        with utils.model_eval(self.model):\n",
    "            features, extra = self.model(\n",
    "                tokens.long().to(device=self.device),\n",
    "                features_only=False,\n",
    "                return_all_hiddens=False,\n",
    "            )\n",
    "        logits = features[0, masked_index, :].squeeze()\n",
    "        prob = logits.softmax(dim=0)\n",
    "        values, index = prob.topk(k=topk, dim=0)\n",
    "        topk_predicted_token_bpe = self.task.source_dictionary.string(index)\n",
    "\n",
    "        topk_filled_outputs = []\n",
    "        for index, predicted_token_bpe in enumerate(\n",
    "            topk_predicted_token_bpe.split(\" \")\n",
    "        ):\n",
    "            predicted_token = self.bpe.decode(predicted_token_bpe)\n",
    "            # Quick hack to fix https://github.com/pytorch/fairseq/issues/1306\n",
    "            if predicted_token_bpe.startswith(\"\\u2581\"):\n",
    "                predicted_token = \" \" + predicted_token\n",
    "            if \" {0}\".format(masked_token) in masked_input:\n",
    "                topk_filled_outputs.append(\n",
    "                    (\n",
    "                        masked_input.replace(\n",
    "                            \" {0}\".format(masked_token), predicted_token\n",
    "                        ),\n",
    "                        values[index].item(),\n",
    "                        predicted_token,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                topk_filled_outputs.append(\n",
    "                    (\n",
    "                        masked_input.replace(masked_token, predicted_token),\n",
    "                        values[index].item(),\n",
    "                        predicted_token,\n",
    "                    )\n",
    "                )\n",
    "        return topk_filled_outputs\n",
    "\n",
    "    def disambiguate_pronoun(self, sentence: str) -> bool:\n",
    "        \"\"\"\n",
    "        Usage::\n",
    "            >>> disambiguate_pronoun('The _trophy_ would not fit in the brown suitcase because [it] was too big.')\n",
    "            True\n",
    "            >>> disambiguate_pronoun('The trophy would not fit in the brown suitcase because [it] was too big.')\n",
    "            'The trophy'\n",
    "        \"\"\"\n",
    "        assert hasattr(\n",
    "            self.task, \"disambiguate_pronoun\"\n",
    "        ), \"roberta.disambiguate_pronoun() requires a model trained with the WSC task.\"\n",
    "        with utils.model_eval(self.model):\n",
    "            return self.task.disambiguate_pronoun(\n",
    "                self.model, sentence, use_cuda=self.device.type == \"cuda\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
