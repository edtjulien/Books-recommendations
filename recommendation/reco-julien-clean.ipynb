{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100) # ‘None’ value means unlimited.\n",
    "\n",
    "THEMES = {\n",
    " 'amitie': 373,\n",
    " 'amour': 77,\n",
    " 'autobiographie': 45,\n",
    " 'aventure': 33,\n",
    " 'bande-dessinee': 18,\n",
    " 'biographie': 31,\n",
    " 'cinema': 89,\n",
    " 'classique': 28,\n",
    " 'comedie-romantique': 3788,\n",
    " 'comics': 142,\n",
    " 'drogue': 863,\n",
    " 'dystopie': 879,\n",
    " 'emotion': 4100,\n",
    " 'enquetes': 3988,\n",
    " 'entretiens': 708,\n",
    " 'essai': 13,\n",
    " 'famille': 290,\n",
    " 'fantastique': 7,\n",
    " 'fantasy': 4,\n",
    " 'geographie': 187,\n",
    " 'guerre': 91,\n",
    " 'humour': 15,\n",
    " 'humour-noir': 621,\n",
    " 'jeunesse': 14,\n",
    " 'journalisme': 975,\n",
    " 'litterature-americaine': 9,\n",
    " 'litterature-asiatique': 1064,\n",
    " 'litterature-francaise': 3,\n",
    " 'manga': 12,\n",
    " 'musique': 44,\n",
    " 'nouvelles': 23,\n",
    " 'peur': 430,\n",
    " 'poesie': 25,\n",
    " 'politique': 54,\n",
    " 'psychologie': 65,\n",
    " 'racisme': 906,\n",
    " 'recit-de-voyage': 448,\n",
    " 'religion': 26,\n",
    " 'reportage': 3488,\n",
    " 'reseaux-sociaux': 26109,\n",
    " 'roman': 1,\n",
    " 'roman-fantastique': 912,\n",
    " 'roman-noir': 136,\n",
    " 'romans-policiers-et-polars': 63883,\n",
    " 'science-fiction': 6,\n",
    " 'sentiments': 1770,\n",
    " 'serie': 788,\n",
    " 'theatre': 21,\n",
    " 'thriller': 11,\n",
    " 'thriller-psychologique': 1073,\n",
    " 'tragedie': 601,\n",
    " 'western': 533\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging files and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_files(books_file_list:list, books_meta_file=None, books_senti_file=None, books_users_file=None):\n",
    "    df_books = None\n",
    "\n",
    "    # fusion of mains books data files with comments\n",
    "    for filename in books_file_list:\n",
    "        df_books_temp = pd.read_json(filename, lines=True)\n",
    "        if df_books is None:\n",
    "            df_books = df_books_temp\n",
    "        else: df_books = pd.concat([df_books, df_books_temp])\n",
    "\n",
    "    df_books = df_books.drop(['tags'],axis=1)\n",
    "\n",
    "    # genre du profil per user_id\n",
    "    if books_users_file is not None:\n",
    "        df_users = pd.read_json(books_users_file, lines=True)\n",
    "        df_books = df_books.merge(df_users, on='user_id', how='left')\n",
    "    df_books = df_books.fillna('')\n",
    "\n",
    "    # join to the meta data books file\n",
    "    if books_meta_file is not None:\n",
    "        df_meta = pd.read_json(books_meta_file, lines=True)\n",
    "        df_meta = df_meta.drop(['book_nb_comm', 'title', 'name', 'surname','img_url','book_date'],axis=1)\n",
    "        df_books = df_books.merge(df_meta, on='book_id', how='inner')\n",
    "\n",
    "    # join sentiments file\n",
    "    if books_senti_file is not None:\n",
    "        df_senti = pd.read_json(books_senti_file, lines=True)\n",
    "        df_senti = df_senti.drop(['title'],axis=1)\n",
    "        df_books = df_books.merge(df_senti, on='book_id', how='left')\n",
    "    df_books = df_books.fillna(0)\n",
    "\n",
    "    return df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comm = merge_data_files(\n",
    "    books_file_list = ['../output/books-julien.json','../output/books-rebecca.json'],\n",
    "    books_meta_file = '../output/books-meta-data.json',\n",
    "    books_senti_file = '../output/vecteurs_sentiments.json',\n",
    "    books_users_file = '../output/users-data.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comm['year'] = df_comm['book_date'].str.extract(r'\\b(\\d{4})\\b').replace('1900','').replace('3889',)\n",
    "# df_comm['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by per book\n",
    "columns_senti = [col for col in df_comm.columns if col.startswith('sen_')]\n",
    "columns_book = ['book_id', 'book_url', 'book_nb_comm', 'title', 'name', 'surname',\n",
    "       'tags', 'img_url', 'book_rating_count', 'book_rating_value',\n",
    "       'book_author_url', 'book_editor', 'book_pages', *columns_senti]\n",
    "\n",
    "def reduce_comm_to_books(df):\n",
    "    return df.copy().groupby(columns_book, as_index=False).count().loc[:,columns_book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = reduce_comm_to_books(df_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now 2 dataset : df_comm and df_books with 2 differents aggregate levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on garde que les tags avec le nom dans filter_list ou si la valeur est supérieur à filter_force_min. ca permet de rétirer les tags rares et peu importants\n",
    "def tags_to_cols(df, col_name, filter_list=None, filter_force_min=24):\n",
    "    df1 = df.copy()\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        tags_as_string = row[col_name]\n",
    "        tags = ast.literal_eval(tags_as_string)\n",
    "\n",
    "        for tag in tags:\n",
    "            if filter_list is not None and not tag[0].strip() in filter_list and tag[1] < filter_force_min:\n",
    "                continue\n",
    "            tag_name = 'tag_'+tag[0].strip().replace(' ','_').lower()\n",
    "            df1.loc[df1.index == index, tag_name] = tag[1]\n",
    "\n",
    "    df1 = df1.fillna(0)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = tags_to_cols(df_books, col_name='tags', filter_list=list(THEMES.keys()), filter_force_min=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.to_json('../output/final/data-books.json',lines=True,orient='records')\n",
    "df_comm.to_json('../output/final/data-comm.json',lines=True,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "class BookReco:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.vectors = []\n",
    "        self.scalars = []\n",
    "        self.predict_scores = None\n",
    "\n",
    "    def add_vector(self, col_prefix, weight=1):\n",
    "        self.vectors.append({'col_name': col_prefix, 'weight': weight})\n",
    "\n",
    "    def add_scalar(self, col_name, weight=1):\n",
    "        self.scalars.append({'col_name': col_name, 'weight': weight})\n",
    "\n",
    "    def __get_cosine_similarity(self, prefix):\n",
    "        data_temp = self.data.filter(regex=f'^{prefix}',axis=1)\n",
    "        data_temp = data_temp.fillna(0)\n",
    "        vec = MinMaxScaler().fit_transform(data_temp)\n",
    "        return cosine_similarity(vec)\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        for i,vector in enumerate(self.vectors):\n",
    "            feats_cs = self.__get_cosine_similarity(vector['col_name'])\n",
    "            self.vectors[i] = {**vector, 'cosine_similar': feats_cs}\n",
    "\n",
    "        for i,scalar in enumerate(self.scalars):\n",
    "            X = self.data.loc[:,[scalar['col_name']]]\n",
    "            X = MinMaxScaler().fit_transform(X)\n",
    "            self.scalars[i] = {**scalar, 'scaled': X.reshape(-1)}\n",
    "\n",
    "    # def __get_cosine_similarity_final(self,scores):\n",
    "    #     X = [list(score) for score in scores]\n",
    "    #     X = MinMaxScaler().fit_transform(X)\n",
    "    #     cs = cosine_similarity(np.array(X).T)\n",
    "    #     return MinMaxScaler().fit_transform(cs)\n",
    "\n",
    "    def predict(self, book_id):\n",
    "        scores = []\n",
    "\n",
    "        try:\n",
    "            index_book = self.data.query('book_id == @book_id').index.values.astype(int)[0]\n",
    "        except:\n",
    "            print(f\"Can't find book_id: {book_id} in the dataset\")\n",
    "            return None\n",
    "        \n",
    "        # get all scores, apply weight\n",
    "        weight_sum = 0 # to normalize at the end, like a mean\n",
    "        for vector in self.vectors:\n",
    "            score = vector['cosine_similar'][index_book]\n",
    "            weight_sum += vector['weight']\n",
    "            scores.append(score*vector['weight'])\n",
    "        \n",
    "        for scalar in self.scalars:\n",
    "            weight_sum += scalar['weight']\n",
    "            scores.append(scalar['scaled']*scalar['weight'])\n",
    "\n",
    "        # sum of all scores\n",
    "        self.predict_scores = None\n",
    "\n",
    "        # cs = self.__get_cosine_similarity_final(scores)\n",
    "        # self.predict_scores = cs[index_book]\n",
    "\n",
    "        for score in scores:\n",
    "            if self.predict_scores is None:\n",
    "                self.predict_scores = np.array(score)\n",
    "            else: self.predict_scores += np.array(score)\n",
    "        \n",
    "        # todo : mettre scores dans un dataframe des scores avec une Serie avec index book_id + faire l'ordre inverse, selection des n premier et minmaxscaler (bof la fin)\n",
    "\n",
    "        # normalisation of sum of all scores\n",
    "        #return MinMaxScaler().fit_transform(self.predict_scores.reshape(-1,1))\n",
    "        #print(self.predict_scores / weight_sum)\n",
    "        return self.predict_scores / weight_sum\n",
    "\n",
    "    def format_prediction(self, scores, max_books):\n",
    "        scores = [(i,bi) for i,bi in enumerate(scores)]\n",
    "        sorted_scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
    "        sorted_scores = sorted_scores[1:max_books+1]\n",
    "        return [(i, self.data.iloc[i,:]['book_id'],self.data.iloc[i,:]['title'],s) for i,s in sorted_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BookReco()\n",
    "br.add_vector('tag_', weight=1)\n",
    "br.add_vector('sen_', weight=10)\n",
    "br.add_scalar('book_rating_value', weight=0)\n",
    "br.add_scalar('book_nb_comm', weight=0)\n",
    "br.fit(df_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1903, 618790, 'Astérix, tome 21 : Le cadeau de César', 0.9686266273452595),\n",
       " (3633,\n",
       "  1177318,\n",
       "  'La boîte à musique, tome 3 : À la recherche des origines',\n",
       "  0.9302639858427326),\n",
       " (1179,\n",
       "  208606,\n",
       "  'A comme Association, Tome 1 : La pâle lumière des ténèbres',\n",
       "  0.9195271672141786),\n",
       " (1550, 451779, 'La tectonique des plaques', 0.9193027880417318),\n",
       " (1576,\n",
       "  463349,\n",
       "  \"Journal d'un (dé)gonflé, tome 2 : Rodrick fait sa loi\",\n",
       "  0.9126579807324091)]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = br.predict(1829) # De Cape et de Crocs, tome 2 : Pavillon noir !\n",
    "br.format_prediction(scores, max_books=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe655022548373991488107ee87292a5acc65b103412eeef527175bfc11a68a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
